{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saria-Alashry/Alashry/blob/%D8%A7%D9%84%D9%83%D9%88%D8%B1%D8%B3-%D8%A7%D9%84%D9%85%D9%81%D8%B5%D9%84-%D9%81%D9%8A-%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D9%88%D9%85%D8%B3%D8%A7%D8%A8%D9%82%D8%A9-%D8%A8%D8%B1%D9%85%D8%AC%D8%A9-50-%D8%A7%D9%84%D9%81-%D8%AC%D9%86%D9%8A%D9%87/week8_sequence_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53TaNcvTCMO"
      },
      "source": [
        "<div dir=\"rtl\" style=\"text-align: right; font-size: 28px; font-weight: bold; background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 10px; margin-bottom: 20px;\">\n",
        "\n",
        "# Week 8: Sequence Models (RNN, LSTM, GRU, BiRNN)\n",
        "\n",
        "النهاردة هنطبقها على Text Classification بإذن الله!\n",
        "\n",
        "</div>\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Instructor:** Dr. Mahmoud Eid (m.eid@fci-cu.edu.eg)\n",
        "\n",
        "**Estimated Time:** 3-4 hours\n",
        "\n",
        "**What You'll Learn:**\n",
        "\n",
        "**Part 1: Many-to-Many Sequence Classification (Food Entity Recognition)**\n",
        "\n",
        "- Word-level classification (is each word food or not?)\n",
        "- Tokenization and pre-trained word embeddings (GloVe)\n",
        "- We will work with Vanilla RNN → GRU → LSTM → BiLSTM\n",
        "\n",
        "**Part 2: Many-to-One Sequence Classification (Spam classification, PRACTICE)**\n",
        "\n",
        "\n",
        "**Prerequisites:** PyTorch basics (Week 5+), Knowledge about text (Lecture 13 and Lecture 14)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gObVdK1TCMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter, defaultdict\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('dark_background')\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmWYW0eSTCMP"
      },
      "source": [
        "---\n",
        "\n",
        "<div dir=\"rtl\" style=\"text-align: right; font-size: 22px; font-weight: bold; background-color: #1a1a2e; padding: 15px; border-radius: 10px;\">\n",
        "\n",
        "# Part 1: Many-to-Many (Food Entity Recognition)\n",
        "\n",
        "</div>\n",
        "\n",
        "**Task:** Given a sentence, classify **each word** as food (1) or not food (0)\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Sentence: `\"I love eating pizza for dinner\"`\n",
        "- Labels: `[0, 0, 0, 1, 0, 0]` (only \"pizza\" is food)\n",
        "\n",
        "This is **Named Entity Recognition (NER)** for food items!\n",
        "\n",
        "\n",
        "## Step 1: Generate Food Entity Dataset\n",
        "\n",
        "We'll create 1200 diverse sentences with 102 unique food items to ensure models **learn patterns** (not just memorize)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jku9ae-FTCMQ"
      },
      "outputs": [],
      "source": [
        "# Food vocabulary (102 unique items across 9 categories)\n",
        "food_items = {\n",
        "    'proteins': ['chicken', 'beef', 'fish', 'salmon', 'tuna', 'turkey', 'lamb',\n",
        "                 'shrimp', 'tofu', 'eggs', 'duck', 'crab', 'lobster'],\n",
        "    'vegetables': ['carrot', 'broccoli', 'spinach', 'tomato', 'onion', 'garlic', 'pepper',\n",
        "                   'lettuce', 'cucumber', 'potato', 'mushroom', 'celery', 'kale', 'zucchini',\n",
        "                   'eggplant', 'avocado', 'beans', 'peas'],\n",
        "    'fruits': ['apple', 'banana', 'orange', 'strawberry', 'blueberry', 'mango', 'pineapple',\n",
        "               'grape', 'watermelon', 'lemon', 'lime', 'peach', 'cherry', 'kiwi', 'berries'],\n",
        "    'grains': ['rice', 'pasta', 'bread', 'quinoa', 'oats', 'noodles', 'tortilla', 'bagel',\n",
        "               'cereal', 'couscous', 'barley', 'crackers'],\n",
        "    'dairy': ['milk', 'cheese', 'yogurt', 'butter', 'cream'],\n",
        "    'beverages': ['coffee', 'tea', 'juice', 'water', 'soda', 'wine', 'beer', 'smoothie',\n",
        "                  'latte', 'lemonade'],\n",
        "    'dishes': ['pizza', 'burger', 'salad', 'soup', 'sandwich', 'curry', 'steak', 'sushi',\n",
        "               'taco', 'burrito', 'ramen', 'lasagna', 'pie', 'omelet', 'pancakes'],\n",
        "    'spices': ['salt', 'pepper', 'cinnamon', 'vanilla', 'oregano', 'basil', 'thyme', 'cumin',\n",
        "               'paprika', 'ginger'],\n",
        "    'sweets': ['chocolate', 'cake', 'cookie', 'candy', 'donut', 'brownie', 'muffin', 'pudding',\n",
        "               'syrup', 'honey'],\n",
        "}\n",
        "\n",
        "all_foods = set([item for category in food_items.values() for item in category])\n",
        "print(f\"Total unique food items: {len(all_foods)}\")\n",
        "print(f\"Sample foods: {list(all_foods)[:15]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz_kmqdOTCMQ"
      },
      "outputs": [],
      "source": [
        "# Sentence templates with diverse structures\n",
        "templates = [\n",
        "    \"I love eating {food} for {meal}\",\n",
        "    \"The restaurant serves delicious {food}\",\n",
        "    \"She ordered {food} and {food} for dinner\",\n",
        "    \"My favorite breakfast is {food} with {food}\",\n",
        "    \"He drinks {beverage} every morning\",\n",
        "    \"The chef prepared {food} with {food}\",\n",
        "    \"Add some {spice} to the {food}\",\n",
        "    \"I bought fresh {food} from the market\",\n",
        "    \"The {food} tastes amazing with {spice}\",\n",
        "    \"She prefers {beverage} over {beverage}\",\n",
        "    \"The menu includes {food} and {food}\",\n",
        "    \"We shared a large {food} last night\",\n",
        "    \"Bake the {food} for thirty minutes\",\n",
        "    \"Season with {spice} and {spice} generously\",\n",
        "    \"Grill the {food} on high heat\",\n",
        "    \"I need to buy {food} and {food}\",\n",
        "    \"The bakery has fresh {food} daily\",\n",
        "    \"My lunch includes {food} and chips\",\n",
        "    \"The kids want {food} for lunch\",\n",
        "    \"Dinner tonight is {food} with {food}\",\n",
        "    \"Mix {food} and {spice} in a bowl\",\n",
        "    \"The {food} was perfectly cooked\",\n",
        "    \"She enjoys {beverage} with her meal\",\n",
        "    \"The store sells organic {food}\",\n",
        "    \"Top it with some {food}\",\n",
        "]\n",
        "\n",
        "meals = ['breakfast', 'lunch', 'dinner', 'snack', 'brunch']\n",
        "\n",
        "\n",
        "def generate_food_sentence():\n",
        "    \"\"\"Generate a random sentence with food entities\"\"\"\n",
        "    template = random.choice(templates)\n",
        "    sentence = template\n",
        "\n",
        "    if '{meal}' in sentence:\n",
        "        sentence = sentence.replace('{meal}', random.choice(meals))\n",
        "\n",
        "    while '{food}' in sentence:\n",
        "        category = random.choice(list(food_items.keys()))\n",
        "        sentence = sentence.replace('{food}', random.choice(food_items[category]), 1)\n",
        "\n",
        "    while '{beverage}' in sentence:\n",
        "        sentence = sentence.replace('{beverage}', random.choice(food_items['beverages']), 1)\n",
        "\n",
        "    while '{spice}' in sentence:\n",
        "        sentence = sentence.replace('{spice}', random.choice(food_items['spices']), 1)\n",
        "\n",
        "    # word-level labels\n",
        "    words = sentence.lower().split()\n",
        "    labels = [1 if word in all_foods else 0 for word in words]\n",
        "\n",
        "    return words, labels\n",
        "\n",
        "\n",
        "# Generate 1200 sentences\n",
        "random.seed(42)\n",
        "all_sentences = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(1200):\n",
        "    words, labels = generate_food_sentence()\n",
        "    all_sentences.append(words)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "# note here how we are adding some samples that express a different meaning than food\n",
        "# so that the models understand from the context that this is not food\n",
        "challenging_context_cases = [\n",
        "    # Animals (not food context)\n",
        "    ([\"the\", \"chicken\", \"crossed\", \"the\", \"road\"], [0, 0, 0, 0, 0]),\n",
        "    ([\"i\", \"saw\", \"a\", \"turkey\", \"in\", \"the\", \"forest\"], [0, 0, 0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"fish\", \"are\", \"swimming\", \"in\", \"the\", \"lake\"], [0, 0, 0, 0, 0, 0, 0]),\n",
        "    ([\"she\", \"bought\", \"a\", \"duck\", \"for\", \"the\", \"pond\"], [0, 0, 0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"lamb\", \"is\", \"grazing\", \"in\", \"the\", \"field\"], [0, 0, 0, 0, 0, 0, 0]),\n",
        "\n",
        "    # Non-food compound words\n",
        "    ([\"the\", \"coffee\", \"table\", \"is\", \"made\", \"of\", \"wood\"], [0, 0, 0, 0, 0, 0, 0]),\n",
        "    ([\"she\", \"opened\", \"a\", \"juice\", \"bar\", \"downtown\"], [0, 0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"pepper\", \"spray\", \"was\", \"very\", \"effective\"], [0, 0, 0, 0, 0, 0]),\n",
        "    ([\"he\", \"wore\", \"a\", \"chocolate\", \"brown\", \"jacket\"], [0, 0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"bread\", \"basket\", \"arrived\", \"empty\"], [0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"apple\", \"tree\", \"is\", \"blooming\"], [0, 0, 0, 0, 0]),\n",
        "    ([\"she\", \"has\", \"strawberry\", \"blonde\", \"hair\"], [0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"milk\", \"bottle\", \"is\", \"in\", \"recycling\"], [0, 0, 0, 0, 0, 0]),\n",
        "    ([\"the\", \"cheese\", \"grater\", \"is\", \"sharp\"], [0, 0, 0, 0, 0]),\n",
        "\n",
        "    # Food context (these are food)\n",
        "    ([\"i\", \"love\", \"eating\", \"grilled\", \"chicken\", \"breast\"], [0, 0, 0, 0, 1, 1]),\n",
        "    ([\"she\", \"ordered\", \"fresh\", \"fish\", \"for\", \"dinner\"], [0, 0, 0, 1, 0, 0]),\n",
        "    ([\"the\", \"roasted\", \"duck\", \"was\", \"delicious\"], [0, 0, 1, 0, 0]),\n",
        "    ([\"i\", \"cooked\", \"lamb\", \"chops\", \"yesterday\"], [0, 0, 1, 0, 0]),\n",
        "    ([\"he\", \"drinks\", \"coffee\", \"every\", \"morning\"], [0, 0, 1, 0, 0]),\n",
        "    ([\"pour\", \"orange\", \"juice\", \"into\", \"the\", \"glass\"], [0, 0, 1, 0, 0, 0]),\n",
        "    ([\"she\", \"spread\", \"strawberry\", \"jam\", \"on\", \"toast\"], [0, 0, 1, 1, 0, 1]),\n",
        "    ([\"the\", \"apple\", \"pie\", \"smells\", \"amazing\"], [0, 1, 1, 0, 0]),\n",
        "    ([\"he\", \"bought\", \"fresh\", \"milk\", \"today\"], [0, 0, 0, 1, 0]),\n",
        "    ([\"grate\", \"some\", \"cheese\", \"on\", \"top\"], [0, 0, 1, 0, 0]),\n",
        "]\n",
        "\n",
        "# adding challenging cases many times to training (so model learns them well)\n",
        "# Repeat 10 times to emphasize these patterns\n",
        "for _ in range(10):\n",
        "    for sentence, labels in challenging_context_cases:\n",
        "        all_sentences.append(sentence)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "# Shuffle all data\n",
        "combined = list(zip(all_sentences, all_labels))  # combined will be a list [ (sentence1, label1), (sentence2, label2), (sentence3, label3), ...]\n",
        "# zip(sentences, labels) -> pairs row by row.\n",
        "random.shuffle(combined)\n",
        "all_sentences, all_labels = zip(*combined)  # we un-pack with * then re-zip across columns\n",
        "# zip(*combined) -> transposes the list of pairs back into two separate lists/tuples (columns).\n",
        "all_sentences = list(all_sentences)\n",
        "all_labels = list(all_labels)\n",
        "\n",
        "# Split: 80% train, 10% validation, 10% test\n",
        "split_idx1 = int(0.8 * len(all_sentences))\n",
        "split_idx2 = int(0.9 * len(all_sentences))\n",
        "\n",
        "train_sentences = all_sentences[:split_idx1]\n",
        "train_labels = all_labels[:split_idx1]\n",
        "val_sentences = all_sentences[split_idx1:split_idx2]\n",
        "val_labels = all_labels[split_idx1:split_idx2]\n",
        "test_sentences = all_sentences[split_idx2:]\n",
        "test_labels = all_labels[split_idx2:]\n",
        "\n",
        "print(f\"Generated {len(all_sentences)} total sentences (includes challenging cases repeated 10x for training)\")\n",
        "print(f\"Train: {len(train_sentences)}, Validation: {len(val_sentences)}, Test: {len(test_sentences)}\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Sample challenging sentences (now part of training!):\")\n",
        "print(\"\\n1. the chicken crossed the road\")\n",
        "print(\"   Labels: [0,0,0,0,0] (chicken = animal, NOT food)\")\n",
        "print(\"\\n2. i love eating grilled chicken breast\")\n",
        "print(\"   Labels: [0,0,0,0,1,1] (chicken = food in this context)\")\n",
        "print(\"\\n3. the coffee table is made of wood\")\n",
        "print(\"   Labels: [0,0,0,0,0,0,0] (coffee table = furniture)\")\n",
        "print(\"\\n4. he drinks coffee every morning\")\n",
        "print(\"   Labels: [0,0,1,0,0] (coffee = beverage)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4esZoZluTCMQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 2: Build Vocabulary and Load Pre-trained Embeddings\n",
        "\n",
        "We'll use **GloVe** (Global Vectors) embeddings trained on billions of words. This gives our models semantic understanding!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NnxOM9lTCMQ"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary from training data\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "word_counts = Counter()\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    word_counts.update(sentence)\n",
        "\n",
        "# Add words that appear at least twice\n",
        "for word, count in word_counts.items():\n",
        "    if count >= 2 and word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample vocab: {list(vocab.items())[:10]}\")\n",
        "\n",
        "# Reverse vocabulary (id -> word)\n",
        "id2word = {idx: word for word, idx in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrk884kzTCMQ"
      },
      "outputs": [],
      "source": [
        "# note in here how i didn't give you the file but i uploaded it to my google drive\n",
        "# so that you can download them to your colab machine\n",
        "# glove_path = 'https://drive.google.com/file/d/1WTbdKo7sNTfvRim2Sxm_aSNn2sCibe1q/view?usp=sharing'\n",
        "import gdown\n",
        "\n",
        "# Google Drive file ID from my link\n",
        "file_id = \"1WTbdKo7sNTfvRim2Sxm_aSNn2sCibe1q\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"glove_vectors.txt\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sguo6yzcTCMR"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained GloVe embeddings\n",
        "# GloVe (Global Vectors) trained on Wikipedia + Gigaword corpus\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "print(\"Loading GloVe embeddings...\")\n",
        "\n",
        "\n",
        "#glove_path = 'glove.2024.wikigiga.100d\\wiki_giga_2024_100_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05.050_combined.txt'\n",
        "glove_path = 'glove_vectors.txt'\n",
        "# Load GloVe vectors\n",
        "glove_vectors = {}\n",
        "skipped = 0\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line_num, line in enumerate(f):\n",
        "        try:\n",
        "            values = line.rstrip().split(' ')\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "\n",
        "            # Only add if vector has correct dimension\n",
        "            if len(vector) == EMBEDDING_DIM:\n",
        "                glove_vectors[word] = vector\n",
        "            else:\n",
        "                skipped += 1\n",
        "        except (ValueError, IndexError):\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        if (line_num + 1) % 100000 == 0:\n",
        "            print(f\"  Processed {line_num + 1:,} lines...\")\n",
        "\n",
        "print(f\"\\nLoaded {len(glove_vectors):,} GloVe vectors\")\n",
        "if skipped > 0:\n",
        "    print(f\"Skipped {skipped:,} malformed lines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtKAj4XiTCMR"
      },
      "outputs": [],
      "source": [
        "# Create embedding matrix for our vocabulary\n",
        "embedding_matrix = np.random.randn(len(vocab), EMBEDDING_DIM).astype('float32') * 0.01\n",
        "\n",
        "# Special tokens\n",
        "embedding_matrix[vocab['<PAD>']] = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "# Fill in GloVe vectors for words in our vocabulary\n",
        "found_count = 0\n",
        "for word, idx in vocab.items():\n",
        "    if word in glove_vectors:\n",
        "        embedding_matrix[idx] = glove_vectors[word]\n",
        "        found_count += 1\n",
        "\n",
        "print(f\"\\nEmbedding matrix shape: {embedding_matrix.shape}\")\n",
        "print(f\"Found pre-trained vectors for {found_count}/{len(vocab)} words ({100*found_count/len(vocab):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWb4F_oFTCMR"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Create PyTorch Dataset and DataLoader\n",
        "\n",
        "We'll use **DataLoader** to efficiently batch and sample data (saves memory!).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87D5Np_WTCMR"
      },
      "outputs": [],
      "source": [
        "class FoodNERDataset(Dataset):\n",
        "    \"\"\"Dataset for food entity recognition\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, labels, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert words to indices\n",
        "        word_ids = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.sentences[idx]]\n",
        "        # if the word is not in vocabulary it returns the ID of the special token <UNK>\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function to pad sequences in a batch together\n",
        "    we pad sequences to equal length so they can be batched into a tensor\n",
        "    also note: for labels, we use padding_value=-1 so the loss function ignores\n",
        "    those positions — so the model doesn't learn from <PAD> tokens\"\"\"\n",
        "    sentences, labels = zip(*batch)\n",
        "\n",
        "    # Get lengths before padding\n",
        "    lengths = torch.tensor([len(s) for s in sentences])\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=vocab['<PAD>'])\n",
        "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1)  # -1 will be ignored in loss\n",
        "\n",
        "    return padded_sentences, padded_labels, lengths\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FoodNERDataset(train_sentences, train_labels, vocab)\n",
        "test_dataset = FoodNERDataset(test_sentences, test_labels, vocab)\n",
        "\n",
        "# Create dataloaders (batch_size=32 for memory efficiency)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"\\nSample batch:\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"  Sentences shape: {sample_batch[0].shape}\")  # (batch_size, max_seq_len)\n",
        "print(f\"  Labels shape: {sample_batch[1].shape}\")\n",
        "print(f\"  Lengths: {sample_batch[2][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-0NJhz5TCMR"
      },
      "source": [
        "---\n",
        "\n",
        "<div dir=\"rtl\" style=\"text-align: right; font-size: 22px; font-weight: bold; background-color: #1a1a2e; padding: 15px; border-radius: 10px;\">\n",
        "\n",
        "## Step 4: Implement the sequence Models\n",
        "\n",
        "</div>\n",
        "\n",
        "We'll implement and compare:\n",
        "\n",
        "1. **Vanilla RNN** - Simple recurrent network\n",
        "2. **GRU** - Gated Recurrent Unit (addresses vanishing gradients)\n",
        "3. **LSTM** - Long Short-Term Memory (better at long-term dependencies)\n",
        "4. **BiLSTM** - Bidirectional LSTM (processes text forward AND backward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEGTvLSSTCMR"
      },
      "outputs": [],
      "source": [
        "# Vanilla RNN Model\n",
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
        "        output, _ = self.rnn(embedded)  # (batch, seq_len, hidden)\n",
        "        predictions = self.fc(output)  # (batch, seq_len, output_dim)\n",
        "        return predictions\n",
        "\n",
        "# GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        predictions = self.fc(output)\n",
        "        return predictions\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        predictions = self.fc(output)\n",
        "        return predictions\n",
        "\n",
        "# BiLSTM Model (processes forward AND backward)\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 because bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)  # (batch, seq_len, hidden*2)\n",
        "        predictions = self.fc(output)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "print(\"  1. Vanilla RNN\")\n",
        "print(\"  2. GRU - Better gradient flow\")\n",
        "print(\"  3. LSTM - Handles long-term dependencies\")\n",
        "print(\"  4. BiLSTM - Sees context from both directions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzjQ94EiTCMR"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, epochs=5, lr=0.001):\n",
        "    \"\"\"Train a sequence model\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore padding\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (sentences, labels, lengths) in enumerate(train_loader):\n",
        "            sentences, labels = sentences.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(sentences)  # (batch, seq_len, 2)\n",
        "\n",
        "            # Reshape for loss computation\n",
        "            predictions = predictions.view(-1, 2)  # (batch*seq_len, 2) .. -1 means the number is not specified\n",
        "            # so that would take the first two dimensions as the new first dimension and the second dimension is 2 (food vs. not food)\n",
        "            labels = labels.view(-1)  # (batch*seq_len,)\n",
        "\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "print(\"Training function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfmtnv87TCMS"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"Evaluate model and return accuracy\"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, labels, lengths in test_loader:\n",
        "            sentences = sentences.to(device)\n",
        "            predictions = model(sentences)\n",
        "            predictions = torch.argmax(predictions, dim=2)  # (batch, seq_len)\n",
        "\n",
        "            # Collect predictions and labels (ignore padding)\n",
        "            for i in range(len(sentences)):\n",
        "                length = lengths[i].item()\n",
        "                all_preds.extend(predictions[i, :length].cpu().numpy())\n",
        "                all_labels.extend(labels[i, :length].numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "print(\"Evaluation function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM4H6s2lTCMS"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 5: Train and Compare All Models\n",
        "\n",
        "Let's train all 4 models and see which performs best!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAeXjaP1TCMS"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 2  # Binary classification (food vs not-food)\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 1. Vanilla RNN\n",
        "print(\"=\"*60)\n",
        "print(\"Training Vanilla RNN...\")\n",
        "print(\"=\"*60)\n",
        "rnn_model = VanillaRNN(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_matrix)\n",
        "rnn_losses = train_model(rnn_model, train_loader, epochs=EPOCHS, lr=LR)\n",
        "rnn_acc = evaluate_model(rnn_model, test_loader)\n",
        "results['Vanilla RNN'] = {'losses': rnn_losses, 'accuracy': rnn_acc}\n",
        "print(f\"\\nVanilla RNN Test Accuracy: {rnn_acc:.4f}\\n\")\n",
        "\n",
        "# 2. GRU\n",
        "print(\"=\"*60)\n",
        "print(\"Training GRU...\")\n",
        "print(\"=\"*60)\n",
        "gru_model = GRUModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_matrix)\n",
        "gru_losses = train_model(gru_model, train_loader, epochs=EPOCHS, lr=LR)\n",
        "gru_acc = evaluate_model(gru_model, test_loader)\n",
        "results['GRU'] = {'losses': gru_losses, 'accuracy': gru_acc}\n",
        "print(f\"\\n✓ GRU Test Accuracy: {gru_acc:.4f}\\n\")\n",
        "\n",
        "# 3. LSTM\n",
        "print(\"=\"*60)\n",
        "print(\"Training LSTM...\")\n",
        "print(\"=\"*60)\n",
        "lstm_model = LSTMModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_matrix)\n",
        "lstm_losses = train_model(lstm_model, train_loader, epochs=EPOCHS, lr=LR)\n",
        "lstm_acc = evaluate_model(lstm_model, test_loader)\n",
        "results['LSTM'] = {'losses': lstm_losses, 'accuracy': lstm_acc}\n",
        "print(f\"\\n✓ LSTM Test Accuracy: {lstm_acc:.4f}\\n\")\n",
        "\n",
        "# 4. BiLSTM\n",
        "print(\"=\"*60)\n",
        "print(\"Training BiLSTM (Bidirectional)...\")\n",
        "print(\"=\"*60)\n",
        "bilstm_model = BiLSTMModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_matrix)\n",
        "bilstm_losses = train_model(bilstm_model, train_loader, epochs=EPOCHS, lr=LR)\n",
        "bilstm_acc = evaluate_model(bilstm_model, test_loader)\n",
        "results['BiLSTM'] = {'losses': bilstm_losses, 'accuracy': bilstm_acc}\n",
        "print(f\"\\n✓ BiLSTM Test Accuracy: {bilstm_acc:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFBVyrapTCMS"
      },
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "for name, data in results.items():\n",
        "    ax1.plot(range(1, EPOCHS+1), data['losses'], marker='o', label=name, linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "models = list(results.keys())\n",
        "accuracies = [results[m]['accuracy'] for m in models]\n",
        "colors = ['cyan', 'yellow', 'magenta', 'lime']\n",
        "ax2.bar(models, accuracies, color=colors, alpha=0.7)\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Test Accuracy Comparison')\n",
        "ax2.set_ylim([min(accuracies) - 0.05, 1.0])\n",
        "ax2.grid(alpha=0.3, axis='y')\n",
        "\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "for name, data in results.items():\n",
        "    print(f\"{name:15s} Accuracy: {data['accuracy']:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otflQsSFTCMS"
      },
      "source": [
        "---\n",
        "\n",
        "## Now let's test the best model (BiLSTM) with your own sentences!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrZYaGkRTCMS"
      },
      "outputs": [],
      "source": [
        "def predict_food_entities(sentence, model, vocab):\n",
        "    \"\"\"\n",
        "    Predict food entities in a custom sentence.\n",
        "\n",
        "    Args:\n",
        "        sentence: String sentence to analyze\n",
        "        model: Trained BiLSTM model\n",
        "        vocab: Vocabulary dictionary\n",
        "\n",
        "    Returns:\n",
        "        List of (word, is_food, confidence) tuples\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    words = sentence.lower().split()\n",
        "    word_ids = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_tensor)  # (1, seq_len, 2)\n",
        "        probabilities = torch.softmax(predictions[0], dim=1)  # (seq_len, 2)\n",
        "        predicted_labels = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
        "        confidences = probabilities[:, 1].cpu().numpy()  # Confidence for \"food\" class\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for word, label, conf in zip(words, predicted_labels, confidences):\n",
        "        results.append((word, bool(label), float(conf)))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def display_food_predictions(sentence, results):\n",
        "    \"\"\"Display predictions in a nice format\"\"\"\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Word':<15} {'Is Food?':<15} {'Confidence':<12}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    for word, is_food, conf in results:\n",
        "        label = \"FOOD\" if is_food else \"NOT FOOD\"\n",
        "        color = '\\033[92m' if is_food else '\\033[91m'  # Green for food, red for not .. don't worry about that this is just for nice visualization\n",
        "        reset = '\\033[0m'\n",
        "        print(f\"{word:<15} {color}{label:<15}{reset} {conf:>6.2%}\")\n",
        "\n",
        "    # Summary\n",
        "    food_words = [word for word, is_food, _ in results if is_food]\n",
        "    if food_words:\n",
        "        print(f\"\\nFood items detected: {', '.join(food_words)}\")\n",
        "    else:\n",
        "        print(f\"\\nNo food items detected\")\n",
        "\n",
        "\n",
        "print(\"testing functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npcJvf2QTCMS"
      },
      "outputs": [],
      "source": [
        "# Test the BiLSTM model with custom sentences!\n",
        "\n",
        "test_sentences_food = [\n",
        "    \"I love eating grilled salmon with rice\",\n",
        "    \"The chicken crossed the road yesterday\",\n",
        "    \"She drinks coffee every morning\",\n",
        "    \"The coffee table is made of wood\",\n",
        "    \"I bought fresh strawberries and milk\",\n",
        "    \"She has strawberry blonde hair\",\n",
        "    \"The chef prepared delicious pasta with tomato sauce\",\n",
        "    \"The pepper spray was very effective\",\n",
        "]\n",
        "\n",
        "\n",
        "for sentence in test_sentences_food:\n",
        "    results = predict_food_entities(sentence, bilstm_model, vocab)\n",
        "    display_food_predictions(sentence, results)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4tkLE1PTCMS"
      },
      "outputs": [],
      "source": [
        "# Try your own sentences!\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST YOUR OWN SENTENCES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Add your custom sentences here:\n",
        "my_sentences = [\n",
        "    \"I grilled some beef and vegetables\",\n",
        "    \"The turkey is walking in the garden\",\n",
        "    \"Youssuf played with his pet duck yesterday in the garden, they had nice time!\"\n",
        "    # Add more sentences to test!\n",
        "]\n",
        "\n",
        "for sentence in my_sentences:\n",
        "    results = predict_food_entities(sentence, bilstm_model, vocab)\n",
        "    display_food_predictions(sentence, results)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dyRdQ-ATCMS"
      },
      "source": [
        "---\n",
        "\n",
        "<div dir=\"rtl\" style=\"text-align: right; font-size: 22px; font-weight: bold; background-color: #8b0000; padding: 15px; border-radius: 10px;\">\n",
        "\n",
        "# Part 2: Many-to-One (SMS Spam Classification) - PRACTICE\n",
        "\n",
        "</div>\n",
        "\n",
        "**Now it's YOUR turn!**\n",
        "\n",
        "**Task:** Classify SMS messages as **spam** (1) or **not spam** (0)\n",
        "\n",
        "**Dataset:** SMS Spam Collection (5574 real text messages)\n",
        "\n",
        "**What you need to do:**\n",
        "\n",
        "1. Build vocabulary from SMS dataset\n",
        "2. Create SpamDataset class (many-to-one, like sentiment anlaysis problems)\n",
        "3. Adapt 4 RNN models for binary classification\n",
        "4. Train all models and compare\n",
        "5. Visualize results\n",
        "\n",
        "**Hints:**\n",
        "\n",
        "- Use the same structure as Food NER but change to many-to-one\n",
        "- For many-to-one: Use final hidden state (not all outputs)\n",
        "- GloVe embeddings are already loaded!\n",
        "- Follow the patterns from the Food NER example above\n",
        "\n",
        "## Load SMS Spam Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCvHuv-ETCMS"
      },
      "outputs": [],
      "source": [
        "# Load SMS Spam dataset\n",
        "print('Loading SMS Spam dataset...')\n",
        "sms_data = load_dataset('sms_spam', split='train')\n",
        "\n",
        "# Shuffle and split\n",
        "sms_data = sms_data.shuffle(seed=42)\n",
        "split_idx = int(0.8 * len(sms_data))\n",
        "\n",
        "train_sms = sms_data.select(range(split_idx))\n",
        "test_sms = sms_data.select(range(split_idx, len(sms_data)))\n",
        "\n",
        "print(f'\\nDataset: SMS Spam Collection')\n",
        "print(f'Train: {len(train_sms)} messages')\n",
        "print(f'Test: {len(test_sms)} messages')\n",
        "print(f'\\nClasses: 0=Not Spam, 1=Spam')\n",
        "\n",
        "print(f'\\nSample messages:')\n",
        "print(f'Message: {train_sms[0][\"sms\"]}')\n",
        "print(f'Label: {train_sms[0][\"label\"]}')\n",
        "print(f'\\nMessage: {train_sms[10][\"sms\"]}')\n",
        "print(f'Label: {train_sms[10][\"label\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zLfQ1KTCMT"
      },
      "source": [
        "## TODO 1: Build Vocabulary for SMS Data\n",
        "\n",
        "Create a vocabulary dictionary from the SMS training data.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Include `<PAD>` and `<UNK>` tokens\n",
        "- Add words appearing at least 2 times\n",
        "- Store result in `sms_vocab` dictionary\n",
        "\n",
        "**Hint:** Follow the same pattern as Food NER vocabulary building!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Rge7bwRDTCMT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1b97516a2b5d22e2ed647441d15de178",
          "grade": false,
          "grade_id": "sms_vocab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# TODO: Build vocabulary from SMS training data\n",
        "# Your code here:\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "print(f'SMS Vocabulary size: {len(sms_vocab)}')\n",
        "print(f'Sample vocab: {list(sms_vocab.items())[:10]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "j7pRUcOWTCMT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dd9d70cec0771925579b61d64808f412",
          "grade": true,
          "grade_id": "test_sms_vocab",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# HIDDEN TESTS for vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dFtdFRwTCMT"
      },
      "outputs": [],
      "source": [
        "# Create SMS embedding matrix (using same GloVe vectors)\n",
        "sms_embedding_matrix = np.random.randn(len(sms_vocab), EMBEDDING_DIM).astype('float32') * 0.01\n",
        "sms_embedding_matrix[sms_vocab['<PAD>']] = np.zeros(EMBEDDING_DIM)\n",
        "found_count_sms = 0\n",
        "for word, idx in sms_vocab.items():\n",
        "    if word in glove_vectors:\n",
        "        sms_embedding_matrix[idx] = glove_vectors[word]\n",
        "        found_count_sms += 1\n",
        "\n",
        "print(f'SMS Embedding matrix shape: {sms_embedding_matrix.shape}')\n",
        "print(f'Found GloVe vectors for {found_count_sms}/{len(sms_vocab)} words ({100*found_count_sms/len(sms_vocab):.1f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px8LfknXTCMT"
      },
      "source": [
        "## TODO 2: Create SpamDataset Class\n",
        "\n",
        "Create a PyTorch Dataset class for SMS spam classification.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Inherit from `torch.utils.data.Dataset`\n",
        "- Implement `__init__`, `__len__`, `__getitem__`\n",
        "- Convert SMS text to word IDs\n",
        "- Return (text_tensor, label_tensor)\n",
        "- This is **many-to-one** (entire message → single label)\n",
        "\n",
        "**Hint:** Very similar to FoodNERDataset but labels are scalars, not sequences!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "kHiBTTYITCMT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3a99bdf341ed7c123894c06a69518123",
          "grade": false,
          "grade_id": "spam_dataset",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# TODO: Create SpamDataset class\n",
        "# Your code here:\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# Collate function for spam data\n",
        "\n",
        "\n",
        "def spam_collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    lengths = torch.tensor([len(t) for t in texts])\n",
        "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=sms_vocab['<PAD>'])\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return padded_texts, labels, lengths\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_spam_dataset = SpamDataset(train_sms, sms_vocab)\n",
        "test_spam_dataset = SpamDataset(test_sms, sms_vocab)\n",
        "\n",
        "train_spam_loader = DataLoader(train_spam_dataset, batch_size=32, shuffle=True, collate_fn=spam_collate_fn)\n",
        "test_spam_loader = DataLoader(test_spam_dataset, batch_size=32, shuffle=False, collate_fn=spam_collate_fn)\n",
        "\n",
        "print(f'Train batches: {len(train_spam_loader)}')\n",
        "print(f'Test batches: {len(test_spam_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_wl0otE5TCMT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dfde0356dea6f281fabab64597a4cc6a",
          "grade": true,
          "grade_id": "test_spam_dataset",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# HIDDEN TESTS for SpamDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDK2VIa5TCMT"
      },
      "source": [
        "## TODO 3: Implement 4 RNN Models for Spam Classification\n",
        "\n",
        "Adapt the 4 RNN models for **binary spam classification (many-to-one)**.\n",
        "\n",
        "**Key differences from Food NER:**\n",
        "\n",
        "- Use **final hidden state** (not all outputs)\n",
        "- Output dimension = 2 (spam vs not-spam)\n",
        "- For BiLSTM: Concatenate forward and backward final hidden states\n",
        "\n",
        "**Models to implement:**\n",
        "\n",
        "1. VanillaRNNSpam\n",
        "2. GRUSpam\n",
        "3. LSTMSpam\n",
        "4. BiLSTMSpam\n",
        "\n",
        "**Hint:** Use `pack_padded_sequence` and extract final hidden states!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "zaNVlA7sTCMT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a4ab248b8fdfdc611485d03cee61ba2",
          "grade": false,
          "grade_id": "spam_models",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# TODO: Implement 4 RNN models for spam classification\n",
        "# Your code here:\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "\n",
        "print('All 4 spam classification models defined!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nxMa1EiMTCMT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "253bad13899d35a99845e4b47ef61ae5",
          "grade": true,
          "grade_id": "test_spam_models",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# HIDDEN TESTS for spam models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ABNrTdVTCMU"
      },
      "source": [
        "## Training and Evaluation Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoZ6NRrvTCMU"
      },
      "outputs": [],
      "source": [
        "def train_spam_model(model, train_loader, epochs=10, lr=0.001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for texts, labels, lengths in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(texts, lengths)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "    return losses\n",
        "\n",
        "\n",
        "def evaluate_spam_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels, lengths in test_loader:\n",
        "            texts = texts.to(device)\n",
        "            predictions = model(texts, lengths)\n",
        "            predicted_labels = torch.argmax(predictions, dim=1).cpu().numpy()\n",
        "            all_preds.extend(predicted_labels)\n",
        "            all_labels.extend(labels.numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "print('Training functions ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sttw3y0aTCMU"
      },
      "source": [
        "## Train All 4 Spam Models\n",
        "\n",
        "Now train all 4 models and compare their performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ipUyOkTCMU"
      },
      "outputs": [],
      "source": [
        "# Train all 4 spam models\n",
        "SPAM_HIDDEN_DIM = 128\n",
        "SPAM_EPOCHS = 10\n",
        "SPAM_LR = 0.001\n",
        "\n",
        "spam_results = {}\n",
        "\n",
        "# Vanilla RNN\n",
        "print('='*60)\n",
        "print('Training Vanilla RNN for Spam...')\n",
        "print('='*60)\n",
        "rnn_spam = VanillaRNNSpam(len(sms_vocab), EMBEDDING_DIM, SPAM_HIDDEN_DIM, sms_embedding_matrix)\n",
        "rnn_spam_losses = train_spam_model(rnn_spam, train_spam_loader, epochs=SPAM_EPOCHS, lr=SPAM_LR)\n",
        "rnn_spam_acc = evaluate_spam_model(rnn_spam, test_spam_loader)\n",
        "spam_results['Vanilla RNN'] = {'losses': rnn_spam_losses, 'accuracy': rnn_spam_acc}\n",
        "print(f'\\nVanilla RNN Spam Accuracy: {rnn_spam_acc:.4f}\\n')\n",
        "\n",
        "# GRU\n",
        "print('='*60)\n",
        "print('Training GRU for Spam...')\n",
        "print('='*60)\n",
        "gru_spam = GRUSpam(len(sms_vocab), EMBEDDING_DIM, SPAM_HIDDEN_DIM, sms_embedding_matrix)\n",
        "gru_spam_losses = train_spam_model(gru_spam, train_spam_loader, epochs=SPAM_EPOCHS, lr=SPAM_LR)\n",
        "gru_spam_acc = evaluate_spam_model(gru_spam, test_spam_loader)\n",
        "spam_results['GRU'] = {'losses': gru_spam_losses, 'accuracy': gru_spam_acc}\n",
        "print(f'\\nGRU Spam Accuracy: {gru_spam_acc:.4f}\\n')\n",
        "\n",
        "# LSTM\n",
        "print('='*60)\n",
        "print('Training LSTM for Spam...')\n",
        "print('='*60)\n",
        "lstm_spam = LSTMSpam(len(sms_vocab), EMBEDDING_DIM, SPAM_HIDDEN_DIM, sms_embedding_matrix)\n",
        "lstm_spam_losses = train_spam_model(lstm_spam, train_spam_loader, epochs=SPAM_EPOCHS, lr=SPAM_LR)\n",
        "lstm_spam_acc = evaluate_spam_model(lstm_spam, test_spam_loader)\n",
        "spam_results['LSTM'] = {'losses': lstm_spam_losses, 'accuracy': lstm_spam_acc}\n",
        "print(f'\\nLSTM Spam Accuracy: {lstm_spam_acc:.4f}\\n')\n",
        "\n",
        "# BiLSTM\n",
        "print('='*60)\n",
        "print('Training BiLSTM for Spam...')\n",
        "print('='*60)\n",
        "bilstm_spam = BiLSTMSpam(len(sms_vocab), EMBEDDING_DIM, SPAM_HIDDEN_DIM, sms_embedding_matrix)\n",
        "bilstm_spam_losses = train_spam_model(bilstm_spam, train_spam_loader, epochs=SPAM_EPOCHS, lr=SPAM_LR)\n",
        "bilstm_spam_acc = evaluate_spam_model(bilstm_spam, test_spam_loader)\n",
        "spam_results['BiLSTM'] = {'losses': bilstm_spam_losses, 'accuracy': bilstm_spam_acc}\n",
        "print(f'\\nBiLSTM Spam Accuracy: {bilstm_spam_acc:.4f}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeN2PebhTCMX"
      },
      "outputs": [],
      "source": [
        "# Visualize spam results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Training losses\n",
        "for name, data in spam_results.items():\n",
        "    ax1.plot(range(1, SPAM_EPOCHS+1), data['losses'], marker='o', label=name, linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('SMS Spam: Training Loss Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Test accuracies\n",
        "models = list(spam_results.keys())\n",
        "accuracies = [spam_results[m]['accuracy'] for m in models]\n",
        "colors = ['cyan', 'yellow', 'magenta', 'lime']\n",
        "ax2.bar(models, accuracies, color=colors, alpha=0.7)\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('SMS Spam: Test Accuracy Comparison')\n",
        "ax2.set_ylim([min(accuracies) - 0.05, 1.0])\n",
        "ax2.grid(alpha=0.3, axis='y')\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('='*60)\n",
        "print('SMS SPAM RESULTS SUMMARY')\n",
        "print('='*60)\n",
        "for name, data in spam_results.items():\n",
        "    print(f\"{name:15s} Accuracy: {data['accuracy']:.4f}\")\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icGgVHMRTCMX"
      },
      "source": [
        "---\n",
        "\n",
        "<div dir=\"rtl\" style=\"text-align: right; font-size: 24px; font-weight: bold; background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 10px;\">\n",
        "\n",
        "مبروك! خلصت Week 8 بنجاح والحمد لله\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "**What We Built This Week:**\n",
        "\n",
        "1. Many-to-many sequence labeling (Food NER)\n",
        "2. Many-to-one sequence classification (SMS Spam)\n",
        "3. 8 total models (4 for each task)\n",
        "4. Complete training and evaluation pipeline\n",
        "\n",
        "**Next:** Week 9 covers Attention mechanisms and Transformers!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}